#!/usr/bin/env python3
"""
RAGå®¡è®¡æŠ¥å‘Šç”Ÿæˆå™¨
åŸºäºSQLiteå®¡è®¡æ•°æ®åº“ç”Ÿæˆè¯¦ç»†çš„å®¡è®¡æŠ¥å‘Š
"""

import sqlite3
import json
import os
from datetime import datetime, timedelta
from pathlib import Path


class RAGAuditReportGenerator:
    """RAGå®¡è®¡æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, db_path: str = "./logs/rag_audit.db"):
        self.db_path = db_path
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"å®¡è®¡æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
    
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect(self.db_path)
    
    def get_basic_statistics(self) -> dict:
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»å¯¹è¯æ•°
        cursor.execute("SELECT COUNT(*) FROM rag_audit_logs WHERE event_type = 'CONVERSATION'")
        stats['total_conversations'] = cursor.fetchone()[0]
        
        # é£é™©çº§åˆ«åˆ†å¸ƒ
        cursor.execute("""
            SELECT risk_level, COUNT(*) 
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION' 
            GROUP BY risk_level
        """)
        risk_distribution = dict(cursor.fetchall())
        stats['risk_distribution'] = risk_distribution
        
        # æ´»è·ƒç”¨æˆ·æ•°
        cursor.execute("""
            SELECT COUNT(DISTINCT user_id) 
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION'
        """)
        stats['active_users'] = cursor.fetchone()[0]
        
        # å¹³å‡æ•æ„Ÿåº¦åˆ†æ•°
        cursor.execute("""
            SELECT AVG(sensitive_score) 
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION'
        """)
        avg_score = cursor.fetchone()[0]
        stats['avg_sensitivity_score'] = round(avg_score, 2) if avg_score else 0
        
        # å¹³å‡å“åº”æ—¶é—´
        cursor.execute("""
            SELECT AVG(response_time_ms) 
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION' AND response_time_ms IS NOT NULL
        """)
        avg_time = cursor.fetchone()[0]
        stats['avg_response_time_ms'] = round(avg_time, 2) if avg_time else 0
        
        conn.close()
        return stats
    
    def get_high_risk_events(self) -> list:
        """è·å–é«˜é£é™©äº‹ä»¶"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT timestamp, user_id, session_id, 
                   substr(user_question, 1, 100) as question_preview,
                   substr(llm_response, 1, 100) as response_preview,
                   sensitive_score, keywords_detected, ip_address
            FROM rag_audit_logs 
            WHERE risk_level = 'HIGH' 
               OR event_type = 'HIGH_RISK_DETECTED'
            ORDER BY timestamp DESC
        """)
        
        events = cursor.fetchall()
        conn.close()
        
        return [
            {
                'timestamp': row[0],
                'user_id': row[1],
                'session_id': row[2],
                'question_preview': row[3],
                'response_preview': row[4],
                'sensitive_score': row[5],
                'keywords_detected': row[6],
                'ip_address': row[7]
            }
            for row in events
        ]
    
    def get_user_activity_analysis(self) -> list:
        """è·å–ç”¨æˆ·æ´»åŠ¨åˆ†æ"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                user_id,
                COUNT(*) as total_questions,
                AVG(sensitive_score) as avg_sensitivity,
                MAX(sensitive_score) as max_sensitivity,
                COUNT(CASE WHEN risk_level = 'HIGH' THEN 1 END) as high_risk_count,
                COUNT(CASE WHEN risk_level = 'MEDIUM' THEN 1 END) as medium_risk_count,
                datetime(MAX(timestamp)) as last_activity
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION'
            GROUP BY user_id
            ORDER BY avg_sensitivity DESC, total_questions DESC
        """)
        
        users = cursor.fetchall()
        conn.close()
        
        return [
            {
                'user_id': row[0],
                'total_questions': row[1],
                'avg_sensitivity': round(row[2], 2) if row[2] else 0,
                'max_sensitivity': row[3],
                'high_risk_count': row[4],
                'medium_risk_count': row[5],
                'last_activity': row[6]
            }
            for row in users
        ]
    
    def get_sensitive_keywords_analysis(self) -> list:
        """è·å–æ•æ„Ÿå…³é”®è¯åˆ†æ"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT keywords_detected, COUNT(*) as count
            FROM rag_audit_logs 
            WHERE keywords_detected IS NOT NULL 
              AND keywords_detected != '' 
              AND keywords_detected != '[]'
            GROUP BY keywords_detected
            ORDER BY count DESC
            LIMIT 20
        """)
        
        keywords = cursor.fetchall()
        conn.close()
        
        # è§£æJSONå…³é”®è¯
        keyword_stats = {}
        for row in keywords:
            try:
                kw_list = json.loads(row[0])
                for kw in kw_list:
                    if kw in keyword_stats:
                        keyword_stats[kw] += row[1]
                    else:
                        keyword_stats[kw] = row[1]
            except:
                continue
        
        return sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)[:15]
    
    def get_time_series_analysis(self, days: int = 7) -> list:
        """è·å–æ—¶é—´åºåˆ—åˆ†æ"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                date(timestamp) as date,
                COUNT(*) as total_conversations,
                AVG(response_time_ms) as avg_response_time,
                AVG(sensitive_score) as avg_sensitivity,
                COUNT(CASE WHEN risk_level = 'HIGH' THEN 1 END) as high_risk_count
            FROM rag_audit_logs 
            WHERE event_type = 'CONVERSATION' 
              AND timestamp >= datetime('now', '-{} days')
            GROUP BY date(timestamp)
            ORDER BY date DESC
        """.format(days))
        
        data = cursor.fetchall()
        conn.close()
        
        return [
            {
                'date': row[0],
                'total_conversations': row[1],
                'avg_response_time': round(row[2], 2) if row[2] else 0,
                'avg_sensitivity': round(row[3], 2) if row[3] else 0,
                'high_risk_count': row[4]
            }
            for row in data
        ]
    
    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆå®¡è®¡æŠ¥å‘Š"""
        # è·å–æ‰€æœ‰æ•°æ®
        stats = self.get_basic_statistics()
        high_risk_events = self.get_high_risk_events()
        user_analysis = self.get_user_activity_analysis()
        keyword_analysis = self.get_sensitive_keywords_analysis()
        time_analysis = self.get_time_series_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# RAGç³»ç»Ÿç»¼åˆå®¡è®¡æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®åº“**: {self.db_path}

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

- **æ€»å¯¹è¯æ•°**: {stats['total_conversations']}
- **æ´»è·ƒç”¨æˆ·æ•°**: {stats['active_users']}
- **å¹³å‡æ•æ„Ÿåº¦åˆ†æ•°**: {stats['avg_sensitivity_score']}
- **å¹³å‡å“åº”æ—¶é—´**: {stats['avg_response_time_ms']}ms

### é£é™©çº§åˆ«åˆ†å¸ƒ

"""
        
        # é£é™©åˆ†å¸ƒ
        for risk_level, count in stats['risk_distribution'].items():
            percentage = (count / stats['total_conversations'] * 100) if stats['total_conversations'] > 0 else 0
            icon = "ğŸŸ¢" if risk_level == "LOW" else "ğŸŸ¡" if risk_level == "MEDIUM" else "ğŸ”´"
            report += f"- {icon} **{risk_level}**: {count} æ¬¡ ({percentage:.1f}%)\n"
        
        # é«˜é£é™©äº‹ä»¶
        report += f"""
## ğŸš¨ é«˜é£é™©äº‹ä»¶ ({len(high_risk_events)} é¡¹)

"""
        
        if high_risk_events:
            for i, event in enumerate(high_risk_events[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                report += f"""### äº‹ä»¶ #{i}
- **æ—¶é—´**: {event['timestamp']}
- **ç”¨æˆ·**: {event['user_id']}
- **æ•æ„Ÿåº¦åˆ†æ•°**: {event['sensitive_score']}
- **æ£€æµ‹å…³é”®è¯**: {event['keywords_detected']}
- **é—®é¢˜é¢„è§ˆ**: {event['question_preview']}...
- **IPåœ°å€**: {event['ip_address'] or 'N/A'}

"""
        else:
            report += "âœ… æš‚æ— é«˜é£é™©äº‹ä»¶\n"
        
        # ç”¨æˆ·æ´»åŠ¨åˆ†æ
        report += f"""
## ğŸ‘¤ ç”¨æˆ·æ´»åŠ¨åˆ†æ

"""
        
        for user in user_analysis[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªç”¨æˆ·
            risk_icon = "ğŸ”´" if user['high_risk_count'] > 0 else "ğŸŸ¡" if user['medium_risk_count'] > 0 else "ğŸŸ¢"
            report += f"""### {risk_icon} ç”¨æˆ·: {user['user_id']}
- **æ€»é—®é¢˜æ•°**: {user['total_questions']}
- **å¹³å‡æ•æ„Ÿåº¦**: {user['avg_sensitivity']}
- **æœ€é«˜æ•æ„Ÿåº¦**: {user['max_sensitivity']}
- **é«˜é£é™©å¯¹è¯**: {user['high_risk_count']} æ¬¡
- **ä¸­é£é™©å¯¹è¯**: {user['medium_risk_count']} æ¬¡
- **æœ€åæ´»åŠ¨**: {user['last_activity']}

"""
        
        # æ•æ„Ÿå…³é”®è¯åˆ†æ
        report += f"""
## ğŸ” æ•æ„Ÿå…³é”®è¯æ£€æµ‹ç»Ÿè®¡

"""
        
        for keyword, count in keyword_analysis[:15]:
            report += f"- **{keyword}**: {count} æ¬¡\n"
        
        # æ—¶é—´åºåˆ—åˆ†æ
        report += f"""
## ğŸ“ˆ æ—¶é—´è¶‹åŠ¿åˆ†æ (æœ€è¿‘7å¤©)

"""
        
        for day_data in time_analysis:
            report += f"""### {day_data['date']}
- **å¯¹è¯æ•°**: {day_data['total_conversations']}
- **å¹³å‡å“åº”æ—¶é—´**: {day_data['avg_response_time']}ms
- **å¹³å‡æ•æ„Ÿåº¦**: {day_data['avg_sensitivity']}
- **é«˜é£é™©äº‹ä»¶**: {day_data['high_risk_count']}

"""
        
        report += f"""
## ğŸ›¡ï¸ å®‰å…¨å»ºè®®

åŸºäºå®¡è®¡åˆ†æï¼Œæå‡ºä»¥ä¸‹å®‰å…¨å»ºè®®ï¼š

"""
        
        # åŸºäºæ•°æ®ç”Ÿæˆå»ºè®®
        if stats['avg_sensitivity_score'] > 2:
            report += "âš ï¸  **é«˜æ•æ„Ÿåº¦è­¦å‘Š**: ç³»ç»Ÿæ•´ä½“æ•æ„Ÿåº¦åé«˜ï¼Œå»ºè®®åŠ å¼ºå†…å®¹è¿‡æ»¤\n"
        
        if len(high_risk_events) > 0:
            report += "ğŸš¨ **é«˜é£é™©äº‹ä»¶å…³æ³¨**: å‘ç°é«˜é£é™©äº‹ä»¶ï¼Œå»ºè®®è¯¦ç»†å®¡æŸ¥ç›¸å…³ç”¨æˆ·è¡Œä¸º\n"
        
        high_risk_users = [u for u in user_analysis if u['high_risk_count'] > 0]
        if high_risk_users:
            report += f"ğŸ‘¤ **é‡ç‚¹ç”¨æˆ·ç›‘æ§**: {len(high_risk_users)} åç”¨æˆ·æœ‰é«˜é£é™©è¡Œä¸ºï¼Œå»ºè®®é‡ç‚¹å…³æ³¨\n"
        
        if stats['avg_response_time_ms'] > 5000:
            report += "â±ï¸  **æ€§èƒ½ä¼˜åŒ–**: å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½\n"
        
        report += """
âœ… **ç³»ç»Ÿè¿è¡Œæ­£å¸¸**: å®¡è®¡æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼ŒæŒç»­ç›‘æ§ä¸­

---

*æ­¤æŠ¥å‘Šç”±RAGå®¡è®¡ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return report


def main():
    """ä¸»å‡½æ•° - ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
    db_path = "./logs/rag_audit.db"
    
    if not os.path.exists(db_path):
        print(f"âŒ å®¡è®¡æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        print("è¯·å…ˆè¿è¡ŒRAGç³»ç»Ÿä»¥ç”Ÿæˆå®¡è®¡æ•°æ®")
        return
    
    try:
        generator = RAGAuditReportGenerator(db_path)
        report = generator.generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = f"./logs/comprehensive_audit_report_{timestamp}.md"
        
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… ç»¼åˆå®¡è®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        stats = generator.get_basic_statistics()
        print("\nğŸ“Š å®¡è®¡æ•°æ®æ¦‚è§ˆ:")
        print(f"   æ€»å¯¹è¯æ•°: {stats['total_conversations']}")
        print(f"   æ´»è·ƒç”¨æˆ·: {stats['active_users']}")
        print(f"   å¹³å‡æ•æ„Ÿåº¦: {stats['avg_sensitivity_score']}")
        print(f"   é£é™©åˆ†å¸ƒ: {stats['risk_distribution']}")
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()