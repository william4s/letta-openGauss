# åŸºäºLettaå’ŒOpenGaussçš„RAGç³»ç»Ÿæ„å»ºæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨Lettaæ¡†æ¶å’ŒOpenGausså‘é‡æ•°æ®åº“æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿã€‚

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è¯¦ç»†æ­¥éª¤](#è¯¦ç»†æ­¥éª¤)
5. [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ–‡æ¡£è¾“å…¥   â”‚â”€â”€â”€â–¶â”‚  æ–‡æœ¬æå–    â”‚â”€â”€â”€â–¶â”‚   åˆ†å—å¤„ç†   â”‚
â”‚  (PDF/TXT)  â”‚    â”‚ (pypdf/å…¶ä»–)  â”‚    â”‚  (æ™ºèƒ½åˆ†å‰²)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â–¼
â”‚ OpenGauss   â”‚â—€â”€â”€â”€â”‚   å‘é‡å­˜å‚¨    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‘é‡æ•°æ®åº“  â”‚    â”‚              â”‚    â”‚  BGE-M3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Embedding  â”‚
      â–²                                â”‚  æ¨¡å‹       â”‚
      â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â–¼
â”‚   Qwen3     â”‚â—€â”€â”€â”€â”‚   ä¸Šä¸‹æ–‡æ„å»º  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMæ¨¡å‹   â”‚    â”‚   RAGèåˆ    â”‚â—€â”€â”€â”€â”‚   å‘é‡æ£€ç´¢   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (ç›¸ä¼¼åº¦æœç´¢) â”‚
      â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼                                       â–²
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚   æ™ºèƒ½å›ç­”   â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç”Ÿæˆè¾“å‡º   â”‚                    â”‚        ç”¨æˆ·é—®é¢˜å¤„ç†          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### 1. åŸºç¡€ä¾èµ–

```bash
# Pythonç¯å¢ƒ (æ¨è3.8+)
pip install letta-client
pip install pypdf  # PDFå¤„ç†
pip install requests  # HTTPè¯·æ±‚
pip install psycopg2-binary  # PostgreSQL/OpenGaussè¿æ¥
```

### 2. æœåŠ¡ç»„ä»¶

#### 2.1 LettaæœåŠ¡å™¨
```bash
# å¯åŠ¨LettaæœåŠ¡å™¨
cd /path/to/letta
python -m letta server

# éªŒè¯æœåŠ¡
curl http://localhost:8283/health
```

#### 2.2 BGE-M3 EmbeddingæœåŠ¡
```bash
# å¯åŠ¨embeddingæœåŠ¡ (ç«¯å£8003)
# ç¡®ä¿BGE-M3æ¨¡å‹å¯é€šè¿‡ä»¥ä¸‹åœ°å€è®¿é—®ï¼š
# http://127.0.0.1:8003/v1/embeddings
```

#### 2.3 Qwen3 LLMæœåŠ¡
```bash
# ç¡®ä¿Qwen3æ¨¡å‹é€šè¿‡Lettaå¯è®¿é—®
# æ¨¡å‹æ ‡è¯†: openai/qwen3
```

#### 2.4 OpenGaussæ•°æ®åº“
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENGAUSS_HOST=localhost
export OPENGAUSS_PORT=5432
export OPENGAUSS_DATABASE=letta
export OPENGAUSS_USERNAME=postgres
export OPENGAUSS_PASSWORD=your_password
```

### 3. éªŒè¯ç¯å¢ƒ

```python
# ä½¿ç”¨æä¾›çš„ç¯å¢ƒæ£€æŸ¥è„šæœ¬
python jr_config_check.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ç°æœ‰ç¤ºä¾‹

```bash
# 1. å¿«é€Ÿæµ‹è¯• (æ¨è)
python direct_embedding_rag.py

# 2. é…ç½®æ£€æŸ¥
python jr_config_check.py

# 3. è°ƒè¯•embedding
python debug_embedding.py
```

### æ–¹å¼äºŒï¼šä»é›¶æ„å»º

#### ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºRAGç±»

```python
import os
import sys
import time
import requests
from pathlib import Path
from typing import List, Dict

from letta_client import Letta, CreateBlock, MessageCreate

class MyRAGSystem:
    def __init__(self):
        self.client = Letta(base_url="http://localhost:8283")
        self.embedding_url = "http://127.0.0.1:8003/v1/embeddings"
        self.source = None
        self.agent = None
        self.text_chunks = []
        self.chunk_embeddings = []
```

#### ç¬¬äºŒæ­¥ï¼šæ–‡æ¡£å¤„ç†

```python
def extract_text_from_pdf(self, pdf_path: str) -> str:
    """æå–PDFæ–‡æœ¬"""
    import pypdf
    
    with open(pdf_path, 'rb') as file:
        reader = pypdf.PdfReader(file)
        full_text = ""
        
        for page in reader.pages:
            full_text += page.extract_text()
    
    return full_text

def chunk_text(self, text: str, chunk_size: int = 300) -> List[str]:
    """æ–‡æœ¬åˆ†å—"""
    sentences = text.split('ã€‚')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + "ã€‚"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + "ã€‚"
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

#### ç¬¬ä¸‰æ­¥ï¼šå‘é‡åŒ–å¤„ç†

```python
def get_embeddings(self, texts: List[str]) -> List[List[float]]:
    """è°ƒç”¨BGE-M3ç”Ÿæˆembeddingå‘é‡"""
    response = requests.post(
        self.embedding_url,
        json={
            "model": "bge-m3",
            "input": texts
        },
        headers={"Content-Type": "application/json"},
        timeout=60
    )
    
    if response.status_code == 200:
        data = response.json()
        return [item['embedding'] for item in data['data']]
    else:
        raise Exception(f"Embeddingè°ƒç”¨å¤±è´¥: {response.status_code}")
```

#### ç¬¬å››æ­¥ï¼šç›¸ä¼¼åº¦æœç´¢

```python
def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import math
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(a * a for a in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0
    
    return dot_product / (magnitude1 * magnitude2)

def similarity_search(self, query: str, top_k: int = 3) -> List[Dict]:
    """æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢"""
    # 1. è·å–æŸ¥è¯¢çš„embedding
    query_embeddings = self.get_embeddings([query])
    query_embedding = query_embeddings[0]
    
    # 2. è®¡ç®—ç›¸ä¼¼åº¦
    similarities = []
    for i, chunk_embedding in enumerate(self.chunk_embeddings):
        similarity = self.cosine_similarity(query_embedding, chunk_embedding)
        similarities.append({
            'index': i,
            'text': self.text_chunks[i],
            'similarity': similarity
        })
    
    # 3. è¿”å›top_kç»“æœ
    similarities.sort(key=lambda x: x['similarity'], reverse=True)
    return similarities[:top_k]
```

#### ç¬¬äº”æ­¥ï¼šåˆ›å»ºRAGæ™ºèƒ½ä½“

```python
def create_rag_agent(self):
    """åˆ›å»ºRAGæ™ºèƒ½ä½“"""
    memory_blocks = [
        CreateBlock(
            value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚",
            label="system_instruction",
        ),
        CreateBlock(
            value=f"å½“å‰å·²åŠ è½½æ–‡æ¡£ï¼Œå…± {len(self.text_chunks)} ä¸ªç‰‡æ®µã€‚",
            label="document_status",
        ),
    ]
    
    self.agent = self.client.agents.create(
        memory_blocks=memory_blocks,
        model="openai/qwen3",        # ä½¿ç”¨Qwen3æ¨¡å‹
        embedding="bge/bge-m3",      # ä½¿ç”¨BGE-M3åµŒå…¥
    )
    
    return self.agent
```

#### ç¬¬å…­æ­¥ï¼šRAGé—®ç­”

```python
def ask_question_with_rag(self, question: str) -> str:
    """ä½¿ç”¨RAGå›ç­”é—®é¢˜"""
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    relevant_docs = self.similarity_search(question, top_k=3)
    
    # 2. æ„å»ºå¢å¼ºçš„prompt
    context = "\\n\\n".join([doc['text'] for doc in relevant_docs])
    enhanced_question = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚"""
    
    # 3. è°ƒç”¨æ™ºèƒ½ä½“
    response = self.client.agents.messages.create(
        agent_id=self.agent.id,
        messages=[
            MessageCreate(
                role="user",
                content=enhanced_question,
            ),
        ],
    )
    
    # 4. æå–å›ç­”
    for msg in response.messages:
        if msg.message_type == "assistant_message":
            return msg.content
    
    return "æœªèƒ½è·å–å›ç­”"
```

## ğŸ“ è¯¦ç»†æ­¥éª¤

### æ­¥éª¤1ï¼šæ–‡æ¡£æºç®¡ç†

#### åˆ›å»ºæ–‡æ¡£æº
```python
# åˆ›å»ºæ–‡æ¡£æºï¼ŒæŒ‡å®šembeddingæ¨¡å‹
source = client.sources.create(
    name="my_knowledge_base",
    embedding="bge/bge-m3",
)
```

#### æ£€æŸ¥embeddingé…ç½®
```python
print(f"Embeddingé…ç½®: {source.embedding_config}")
# è¾“å‡ºç¤ºä¾‹:
# embedding_endpoint_type='openai' 
# embedding_endpoint='http://127.0.0.1:8003/v1' 
# embedding_model='bge-m3' 
# embedding_dim=1024 
# embedding_chunk_size=300
```

### æ­¥éª¤2ï¼šæ–‡æœ¬é¢„å¤„ç†

#### æ™ºèƒ½åˆ†å—ç­–ç•¥
```python
def advanced_chunk_text(text: str, strategy: str = "semantic") -> List[str]:
    """é«˜çº§æ–‡æœ¬åˆ†å—"""
    if strategy == "semantic":
        # åŸºäºè¯­ä¹‰çš„åˆ†å—
        return semantic_chunking(text)
    elif strategy == "fixed":
        # å›ºå®šé•¿åº¦åˆ†å—
        return fixed_length_chunking(text, chunk_size=300)
    elif strategy == "sentence":
        # åŸºäºå¥å­çš„åˆ†å—
        return sentence_based_chunking(text)
    else:
        return simple_chunking(text)

def semantic_chunking(text: str) -> List[str]:
    """è¯­ä¹‰åˆ†å— - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\\n\\n')
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) < 500:  # è¯­ä¹‰å—å¯ä»¥æ›´é•¿
            current_chunk += para + "\\n\\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\\n\\n"
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

### æ­¥éª¤3ï¼šå‘é‡å­˜å‚¨ä¼˜åŒ–

#### æ‰¹é‡å¤„ç†
```python
def batch_process_embeddings(self, texts: List[str], batch_size: int = 32):
    """æ‰¹é‡å¤„ç†embedding"""
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_embeddings = self.get_embeddings(batch)
        all_embeddings.extend(batch_embeddings)
        
        print(f"å¤„ç†è¿›åº¦: {min(i + batch_size, len(texts))}/{len(texts)}")
        time.sleep(0.1)  # é¿å…APIé™æµ
    
    return all_embeddings
```

#### å‘é‡æŒä¹…åŒ–
```python
def save_embeddings_to_db(self, chunks: List[str], embeddings: List[List[float]]):
    """ä¿å­˜å‘é‡åˆ°OpenGaussæ•°æ®åº“"""
    import psycopg2
    
    conn = psycopg2.connect(
        host=os.getenv('OPENGAUSS_HOST'),
        port=os.getenv('OPENGAUSS_PORT'),
        database=os.getenv('OPENGAUSS_DATABASE'),
        user=os.getenv('OPENGAUSS_USERNAME'),
        password=os.getenv('OPENGAUSS_PASSWORD')
    )
    
    cursor = conn.cursor()
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS document_embeddings (
            id SERIAL PRIMARY KEY,
            text TEXT NOT NULL,
            embedding FLOAT[] NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # æ‰¹é‡æ’å…¥
    for chunk, embedding in zip(chunks, embeddings):
        cursor.execute(
            "INSERT INTO document_embeddings (text, embedding) VALUES (%s, %s)",
            (chunk, embedding)
        )
    
    conn.commit()
    cursor.close()
    conn.close()
```

### æ­¥éª¤4ï¼šæ£€ç´¢ä¼˜åŒ–

#### æ··åˆæ£€ç´¢ç­–ç•¥
```python
def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:
    """æ··åˆæ£€ç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢"""
    # 1. å‘é‡æœç´¢
    vector_results = self.similarity_search(query, top_k=top_k*2)
    
    # 2. å…³é”®è¯æœç´¢
    keyword_results = self.keyword_search(query, top_k=top_k*2)
    
    # 3. ç»“æœèåˆå’Œé‡æ’åº
    combined_results = self.rerank_results(vector_results, keyword_results)
    
    return combined_results[:top_k]

def keyword_search(self, query: str, top_k: int) -> List[Dict]:
    """å…³é”®è¯æœç´¢"""
    keywords = query.split()
    results = []
    
    for i, chunk in enumerate(self.text_chunks):
        score = 0
        for keyword in keywords:
            if keyword in chunk:
                score += chunk.count(keyword)
        
        if score > 0:
            results.append({
                'index': i,
                'text': chunk,
                'keyword_score': score
            })
    
    results.sort(key=lambda x: x['keyword_score'], reverse=True)
    return results[:top_k]
```

### æ­¥éª¤5ï¼šæ™ºèƒ½ä½“å¢å¼º

#### æ·»åŠ ä¸“ç”¨å·¥å…·
```python
def create_enhanced_tools(self):
    """åˆ›å»ºå¢å¼ºçš„RAGå·¥å…·"""
    
    def search_documents(query: str, top_k: int = 3) -> str:
        """åœ¨æ–‡æ¡£ä¸­æœç´¢"""
        results = self.similarity_search(query, top_k)
        return "\\n".join([f"ç›¸å…³åº¦{r['similarity']:.3f}: {r['text']}" 
                          for r in results])
    
    def summarize_section(keywords: str) -> str:
        """æ€»ç»“åŒ…å«ç‰¹å®šå…³é”®è¯çš„éƒ¨åˆ†"""
        relevant_chunks = [chunk for chunk in self.text_chunks 
                          if any(kw in chunk for kw in keywords.split())]
        
        if not relevant_chunks:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        # ä½¿ç”¨LLMæ€»ç»“
        summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\\n\\n" + "\\n\\n".join(relevant_chunks[:3])
        return self.call_llm_for_summary(summary_prompt)
    
    def extract_entities(text_type: str = "all") -> str:
        """æå–å®ä½“ä¿¡æ¯"""
        entities = {"äººå": [], "åœ°å": [], "ç»„ç»‡": [], "æ—¶é—´": []}
        
        # ç®€å•çš„å®ä½“æå–é€»è¾‘
        for chunk in self.text_chunks[:5]:  # åªå¤„ç†å‰5ä¸ªå—
            # è¿™é‡Œå¯ä»¥é›†æˆNERæ¨¡å‹
            pass
        
        return str(entities)
    
    # æ³¨å†Œå·¥å…·
    search_tool = self.client.tools.upsert_from_function(func=search_documents)
    summary_tool = self.client.tools.upsert_from_function(func=summarize_section)
    entity_tool = self.client.tools.upsert_from_function(func=extract_entities)
    
    # é™„åŠ åˆ°æ™ºèƒ½ä½“
    for tool in [search_tool, summary_tool, entity_tool]:
        self.client.agents.tools.attach(
            agent_id=self.agent.id,
            tool_id=tool.id
        )
```

## âš™ï¸ é«˜çº§é…ç½®

### 1. æ€§èƒ½ä¼˜åŒ–

#### ç¼“å­˜æœºåˆ¶
```python
import hashlib
import pickle
import os

class EmbeddingCache:
    def __init__(self, cache_dir="./embedding_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode()).hexdigest()
    
    def get(self, text: str) -> List[float]:
        cache_key = self.get_cache_key(text)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, text: str, embedding: List[float]):
        cache_key = self.get_cache_key(text)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        with open(cache_file, 'wb') as f:
            pickle.dump(embedding, f)

# ä½¿ç”¨ç¼“å­˜
cache = EmbeddingCache()

def get_embeddings_with_cache(self, texts: List[str]) -> List[List[float]]:
    embeddings = []
    uncached_texts = []
    uncached_indices = []
    
    for i, text in enumerate(texts):
        cached_emb = cache.get(text)
        if cached_emb:
            embeddings.append(cached_emb)
        else:
            embeddings.append(None)
            uncached_texts.append(text)
            uncached_indices.append(i)
    
    # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
    if uncached_texts:
        new_embeddings = self.get_embeddings(uncached_texts)
        for idx, emb in zip(uncached_indices, new_embeddings):
            embeddings[idx] = emb
            cache.set(texts[idx], emb)
    
    return embeddings
```

#### å¹¶è¡Œå¤„ç†
```python
from concurrent.futures import ThreadPoolExecutor
import threading

class ParallelRAG:
    def __init__(self):
        self.lock = threading.Lock()
        self.max_workers = 4
    
    def parallel_embedding(self, text_batches: List[List[str]]) -> List[List[float]]:
        """å¹¶è¡Œå¤„ç†embedding"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.get_embeddings, batch) 
                      for batch in text_batches]
            
            all_embeddings = []
            for future in futures:
                batch_embeddings = future.result()
                all_embeddings.extend(batch_embeddings)
        
        return all_embeddings
```

### 2. è´¨é‡è¯„ä¼°

#### æ£€ç´¢è´¨é‡è¯„ä¼°
```python
def evaluate_retrieval_quality(self, test_queries: List[Dict]):
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""
    results = {
        'precision_at_k': [],
        'recall_at_k': [],
        'mrr': []  # Mean Reciprocal Rank
    }
    
    for query_data in test_queries:
        query = query_data['query']
        relevant_docs = query_data['relevant_doc_indices']
        
        # æ£€ç´¢ç»“æœ
        retrieved = self.similarity_search(query, top_k=10)
        retrieved_indices = [r['index'] for r in retrieved]
        
        # è®¡ç®—æŒ‡æ ‡
        precision = len(set(retrieved_indices) & set(relevant_docs)) / len(retrieved_indices)
        recall = len(set(retrieved_indices) & set(relevant_docs)) / len(relevant_docs)
        
        # MRRè®¡ç®—
        for i, idx in enumerate(retrieved_indices):
            if idx in relevant_docs:
                mrr = 1.0 / (i + 1)
                break
        else:
            mrr = 0.0
        
        results['precision_at_k'].append(precision)
        results['recall_at_k'].append(recall)
        results['mrr'].append(mrr)
    
    # å¹³å‡æŒ‡æ ‡
    avg_precision = sum(results['precision_at_k']) / len(results['precision_at_k'])
    avg_recall = sum(results['recall_at_k']) / len(results['recall_at_k'])
    avg_mrr = sum(results['mrr']) / len(results['mrr'])
    
    print(f"å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.3f}")
    print(f"å¹³å‡å¬å›ç‡: {avg_recall:.3f}")
    print(f"å¹³å‡å€’æ•°æ’å: {avg_mrr:.3f}")
    
    return results
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è§£å†³

#### 1. EmbeddingæœåŠ¡è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://127.0.0.1:8003/v1/models

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep 8003

# é‡å¯æœåŠ¡
# æŒ‰ç…§æ‚¨çš„embeddingæœåŠ¡å¯åŠ¨æ–¹å¼é‡å¯
```

#### 2. å‘é‡ç»´åº¦ä¸åŒ¹é…
```python
def check_embedding_dimensions(self):
    """æ£€æŸ¥embeddingç»´åº¦ä¸€è‡´æ€§"""
    if not self.chunk_embeddings:
        return True
    
    expected_dim = len(self.chunk_embeddings[0])
    for i, emb in enumerate(self.chunk_embeddings):
        if len(emb) != expected_dim:
            print(f"è­¦å‘Š: ç¬¬{i}ä¸ªå‘é‡ç»´åº¦ä¸åŒ¹é…: {len(emb)} vs {expected_dim}")
            return False
    
    print(f"âœ… æ‰€æœ‰å‘é‡ç»´åº¦ä¸€è‡´: {expected_dim}")
    return True
```

#### 3. å†…å­˜ä½¿ç”¨è¿‡å¤š
```python
def optimize_memory_usage(self):
    """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    # 1. åˆ†æ‰¹å¤„ç†å¤§æ–‡æ¡£
    def process_large_document(self, text: str, max_chunk_size: int = 1000):
        if len(text) > max_chunk_size * 100:  # å¦‚æœæ–‡æ¡£å¾ˆå¤§
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(text), max_chunk_size * 50):
                chunk_text = text[i:i + max_chunk_size * 50]
                yield self.chunk_text(chunk_text)
    
    # 2. æ¸…ç†ä¸éœ€è¦çš„æ•°æ®
    def cleanup_memory(self):
        import gc
        # æ¸…ç†ä¸´æ—¶å˜é‡
        gc.collect()
```

#### 4. æœç´¢ç»“æœè´¨é‡å·®
```python
def improve_search_quality(self):
    """æ”¹è¿›æœç´¢è´¨é‡"""
    # 1. è°ƒæ•´åˆ†å—ç­–ç•¥
    self.text_chunks = self.chunk_text(
        self.full_text, 
        chunk_size=200,  # å‡å°å—å¤§å°
        overlap=50       # å¢åŠ é‡å 
    )
    
    # 2. æŸ¥è¯¢æ‰©å±•
    def expand_query(self, query: str) -> str:
        # æ·»åŠ åŒä¹‰è¯ã€ç›¸å…³è¯
        synonyms = {
            "é£é™©": ["å±é™©", "é£é™©æ€§", "ä¸ç¡®å®šæ€§"],
            "æ”¶ç›Š": ["å›æŠ¥", "æ”¶å…¥", "åˆ©æ¶¦", "ç›ˆåˆ©"],
            "æŠ•èµ„": ["ç†è´¢", "æŠ•å…¥", "èµ„é‡‘é…ç½®"]
        }
        
        expanded_terms = [query]
        for word, syns in synonyms.items():
            if word in query:
                expanded_terms.extend(syns)
        
        return " ".join(expanded_terms)
    
    # 3. é‡æ’åº
    def rerank_by_relevance(self, results: List[Dict], query: str) -> List[Dict]:
        # åŸºäºæŸ¥è¯¢è¯å‡ºç°é¢‘ç‡é‡æ’åº
        query_terms = query.split()
        
        for result in results:
            term_count = sum(result['text'].count(term) for term in query_terms)
            result['relevance_boost'] = term_count * 0.1
            result['final_score'] = result['similarity'] + result['relevance_boost']
        
        return sorted(results, key=lambda x: x['final_score'], reverse=True)
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. æ–‡æ¡£å‡†å¤‡
- **æ¸…ç†æ–‡æœ¬**: å»é™¤æ— å…³å­—ç¬¦ã€æ ¼å¼åŒ–é—®é¢˜
- **ç»“æ„åŒ–å¤„ç†**: ä¿æŒç« èŠ‚ã€æ®µè½ç»“æ„
- **å…ƒæ•°æ®æå–**: ä¿å­˜æ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸç­‰ä¿¡æ¯

### 2. åˆ†å—ç­–ç•¥
- **è¯­ä¹‰å®Œæ•´æ€§**: é¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²
- **é€‚å½“é‡å **: 15-20%é‡å é¿å…ä¿¡æ¯ä¸¢å¤±
- **å¤§å°å¹³è¡¡**: 100-500å­—ç¬¦é€šå¸¸æ•ˆæœè¾ƒå¥½

### 3. å‘é‡è´¨é‡
- **æ¨¡å‹é€‰æ‹©**: BGE-M3å¯¹ä¸­æ–‡æ•ˆæœå¥½
- **ç»´åº¦åŒ¹é…**: ç¡®ä¿æ‰€æœ‰å‘é‡ç»´åº¦ä¸€è‡´
- **æ ‡å‡†åŒ–**: è€ƒè™‘L2æ ‡å‡†åŒ–å‘é‡

### 4. æ£€ç´¢ä¼˜åŒ–
- **å¤šç­–ç•¥èåˆ**: å‘é‡+å…³é”®è¯+è¯­ä¹‰æœç´¢
- **åŠ¨æ€è°ƒæ•´**: æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´ç­–ç•¥
- **ç»“æœå¤šæ ·æ€§**: é¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„ç»“æœ

### 5. ç”Ÿæˆè´¨é‡
- **ä¸Šä¸‹æ–‡æ§åˆ¶**: é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦é¿å…æ··ä¹±
- **æç¤ºå·¥ç¨‹**: ç²¾å¿ƒè®¾è®¡ç³»ç»Ÿæç¤ºè¯
- **ä¸€è‡´æ€§æ£€æŸ¥**: éªŒè¯å›ç­”ä¸æ–‡æ¡£çš„ä¸€è‡´æ€§

### 6. ç³»ç»Ÿç›‘æ§
```python
class RAGMonitor:
    def __init__(self):
        self.metrics = {
            'query_count': 0,
            'avg_response_time': 0,
            'cache_hit_rate': 0,
            'embedding_calls': 0
        }
    
    def log_query(self, query: str, response_time: float, cache_hit: bool):
        self.metrics['query_count'] += 1
        self.metrics['avg_response_time'] = (
            (self.metrics['avg_response_time'] * (self.metrics['query_count'] - 1) + response_time) 
            / self.metrics['query_count']
        )
        if cache_hit:
            self.metrics['cache_hit_rate'] += 1
    
    def get_stats(self):
        hit_rate = self.metrics['cache_hit_rate'] / max(self.metrics['query_count'], 1)
        return {
            'total_queries': self.metrics['query_count'],
            'avg_response_time': f"{self.metrics['avg_response_time']:.2f}s",
            'cache_hit_rate': f"{hit_rate:.2%}",
            'embedding_api_calls': self.metrics['embedding_calls']
        }
```

## ğŸ¯ å®Œæ•´ç¤ºä¾‹

### ç”Ÿäº§çº§RAGç³»ç»Ÿæ¨¡æ¿

```python
#!/usr/bin/env python3
"""
ç”Ÿäº§çº§RAGç³»ç»Ÿæ¨¡æ¿
åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†ã€ç›‘æ§ã€ç¼“å­˜ç­‰åŠŸèƒ½
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    letta_base_url: str = "http://localhost:8283"
    embedding_url: str = "http://127.0.0.1:8003/v1/embeddings"
    embedding_model: str = "bge-m3"
    llm_model: str = "openai/qwen3"
    chunk_size: int = 300
    chunk_overlap: int = 50
    top_k: int = 3
    enable_cache: bool = True
    cache_dir: str = "./cache"
    max_retries: int = 3
    timeout: int = 60

class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""
    
    def __init__(self, config: RAGConfig = None):
        self.config = config or RAGConfig()
        self.setup_logging()
        self.setup_cache()
        self.setup_monitoring()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.client = None
        self.agent = None
        self.source = None
        self.embeddings_cache = {}
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        self.logger = logging.getLogger(f"{__name__}.ProductionRAG")
        
    def setup_cache(self):
        """è®¾ç½®ç¼“å­˜"""
        if self.config.enable_cache:
            os.makedirs(self.config.cache_dir, exist_ok=True)
            
    def setup_monitoring(self):
        """è®¾ç½®ç›‘æ§"""
        self.metrics = {
            'start_time': time.time(),
            'queries_processed': 0,
            'embedding_calls': 0,
            'cache_hits': 0,
            'errors': 0
        }
    
    def get_health_status(self) -> Dict:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        uptime = time.time() - self.metrics['start_time']
        
        return {
            'status': 'healthy',
            'uptime_seconds': uptime,
            'metrics': self.metrics,
            'config': {
                'embedding_model': self.config.embedding_model,
                'llm_model': self.config.llm_model,
                'chunk_size': self.config.chunk_size,
                'top_k': self.config.top_k
            }
        }
    
    # ... å…¶ä»–æ–¹æ³•å®ç° ...

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé…ç½®
    config = RAGConfig(
        chunk_size=200,
        top_k=5,
        enable_cache=True
    )
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = ProductionRAG(config)
    
    # å¤„ç†æ–‡æ¡£
    rag.process_document("./your_document.pdf")
    
    # é—®ç­”
    answer = rag.ask("æ‚¨çš„é—®é¢˜")
    print(answer)
    
    # è·å–çŠ¶æ€
    status = rag.get_health_status()
    print(json.dumps(status, indent=2, ensure_ascii=False))
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

åŸºäºJR.PDFæ–‡æ¡£çš„æµ‹è¯•ç»“æœï¼š

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| æ–‡æ¡£å¤„ç†æ—¶é—´ | ~10ç§’ | 7é¡µPDFï¼Œ9589å­—ç¬¦ |
| å‘é‡ç”Ÿæˆæ—¶é—´ | ~15ç§’ | 33ä¸ªæ–‡æ¡£ç‰‡æ®µ |
| æŸ¥è¯¢å“åº”æ—¶é—´ | ~2ç§’ | åŒ…å«æ£€ç´¢+ç”Ÿæˆ |
| æ£€ç´¢ç²¾åº¦ | 0.65-0.75 | ä½™å¼¦ç›¸ä¼¼åº¦ |
| å‘é‡ç»´åº¦ | 1024 | BGE-M3æ ‡å‡† |
| å†…å­˜ä½¿ç”¨ | ~500MB | åŒ…å«æ‰€æœ‰å‘é‡ |

## ğŸ”— å‚è€ƒèµ„æº

- [Lettaå®˜æ–¹æ–‡æ¡£](https://docs.letta.com/)
- [BGE-M3æ¨¡å‹](https://huggingface.co/BAAI/bge-m3)
- [OpenGausså‘é‡æ‰©å±•](https://opengauss.org/)
- [RAGæœ€ä½³å®è·µ](https://arxiv.org/abs/2005.11401)

---

**æ³¨æ„**: æœ¬æŒ‡å—åŸºäºå®é™…æµ‹è¯•çš„å¯å·¥ä½œä»£ç ç¼–å†™ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½ç»è¿‡éªŒè¯ã€‚å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ`letta/examples/`ç›®å½•ä¸‹çš„å®Œæ•´ä»£ç ç¤ºä¾‹ã€‚
